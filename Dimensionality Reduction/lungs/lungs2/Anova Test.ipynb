{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import VarianceThreshold, f_classif, SelectKBest\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2694, 106)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('image_bins_stats_lungs2.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Bins0</th>\n",
       "      <th>Bins1</th>\n",
       "      <th>Bins2</th>\n",
       "      <th>Bins3</th>\n",
       "      <th>Bins4</th>\n",
       "      <th>Bins5</th>\n",
       "      <th>Bins6</th>\n",
       "      <th>Bins7</th>\n",
       "      <th>rmean_bins0</th>\n",
       "      <th>...</th>\n",
       "      <th>bskew_bins7</th>\n",
       "      <th>bkurto_bins0</th>\n",
       "      <th>bkurto_bins1</th>\n",
       "      <th>bkurto_bins2</th>\n",
       "      <th>bkurto_bins3</th>\n",
       "      <th>bkurto_bins4</th>\n",
       "      <th>bkurto_bins5</th>\n",
       "      <th>bkurto_bins6</th>\n",
       "      <th>bkurto_bins7</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformed_image_normal_1.png</td>\n",
       "      <td>5822</td>\n",
       "      <td>14402</td>\n",
       "      <td>0</td>\n",
       "      <td>9189</td>\n",
       "      <td>10538</td>\n",
       "      <td>0</td>\n",
       "      <td>11831</td>\n",
       "      <td>13754</td>\n",
       "      <td>86.982480</td>\n",
       "      <td>...</td>\n",
       "      <td>30.9217012</td>\n",
       "      <td>27.556681</td>\n",
       "      <td>26.125116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.668177</td>\n",
       "      <td>19.678833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.325143</td>\n",
       "      <td>33.029938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transformed_image_normal_2.png</td>\n",
       "      <td>6325</td>\n",
       "      <td>11758</td>\n",
       "      <td>0</td>\n",
       "      <td>10893</td>\n",
       "      <td>13314</td>\n",
       "      <td>0</td>\n",
       "      <td>8460</td>\n",
       "      <td>14786</td>\n",
       "      <td>86.937708</td>\n",
       "      <td>...</td>\n",
       "      <td>32.45653958</td>\n",
       "      <td>30.429971</td>\n",
       "      <td>21.410499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.701043</td>\n",
       "      <td>16.311605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.183742</td>\n",
       "      <td>36.344060</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transformed_image_normal_3.png</td>\n",
       "      <td>5911</td>\n",
       "      <td>15666</td>\n",
       "      <td>0</td>\n",
       "      <td>9025</td>\n",
       "      <td>13587</td>\n",
       "      <td>0</td>\n",
       "      <td>8205</td>\n",
       "      <td>13142</td>\n",
       "      <td>76.833362</td>\n",
       "      <td>...</td>\n",
       "      <td>32.04297751</td>\n",
       "      <td>31.149385</td>\n",
       "      <td>23.374875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.936013</td>\n",
       "      <td>17.187983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.580706</td>\n",
       "      <td>35.631835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformed_image_normal_4.png</td>\n",
       "      <td>5688</td>\n",
       "      <td>15515</td>\n",
       "      <td>0</td>\n",
       "      <td>8941</td>\n",
       "      <td>10462</td>\n",
       "      <td>0</td>\n",
       "      <td>11009</td>\n",
       "      <td>13921</td>\n",
       "      <td>73.869726</td>\n",
       "      <td>...</td>\n",
       "      <td>29.69319943</td>\n",
       "      <td>31.537897</td>\n",
       "      <td>22.614153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.457708</td>\n",
       "      <td>15.659193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.081886</td>\n",
       "      <td>32.843335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transformed_image_normal_5.png</td>\n",
       "      <td>6101</td>\n",
       "      <td>12231</td>\n",
       "      <td>0</td>\n",
       "      <td>10258</td>\n",
       "      <td>14797</td>\n",
       "      <td>0</td>\n",
       "      <td>7655</td>\n",
       "      <td>14494</td>\n",
       "      <td>87.165219</td>\n",
       "      <td>...</td>\n",
       "      <td>32.21430004</td>\n",
       "      <td>31.326113</td>\n",
       "      <td>21.152459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.285766</td>\n",
       "      <td>13.344069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.956341</td>\n",
       "      <td>36.139280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  Bins0  Bins1  Bins2  Bins3  Bins4  Bins5  \\\n",
       "0  transformed_image_normal_1.png   5822  14402      0   9189  10538      0   \n",
       "1  transformed_image_normal_2.png   6325  11758      0  10893  13314      0   \n",
       "2  transformed_image_normal_3.png   5911  15666      0   9025  13587      0   \n",
       "3  transformed_image_normal_4.png   5688  15515      0   8941  10462      0   \n",
       "4  transformed_image_normal_5.png   6101  12231      0  10258  14797      0   \n",
       "\n",
       "   Bins6  Bins7  rmean_bins0  ...  bskew_bins7  bkurto_bins0  bkurto_bins1  \\\n",
       "0  11831  13754    86.982480  ...   30.9217012     27.556681     26.125116   \n",
       "1   8460  14786    86.937708  ...  32.45653958     30.429971     21.410499   \n",
       "2   8205  13142    76.833362  ...  32.04297751     31.149385     23.374875   \n",
       "3  11009  13921    73.869726  ...  29.69319943     31.537897     22.614153   \n",
       "4   7655  14494    87.165219  ...  32.21430004     31.326113     21.152459   \n",
       "\n",
       "   bkurto_bins2  bkurto_bins3  bkurto_bins4  bkurto_bins5 bkurto_bins6  \\\n",
       "0           0.0     12.668177     19.678833           0.0    42.325143   \n",
       "1           0.0      9.701043     16.311605           0.0    41.183742   \n",
       "2           0.0     12.936013     17.187983           0.0    45.580706   \n",
       "3           0.0     11.457708     15.659193           0.0    42.081886   \n",
       "4           0.0     12.285766     13.344069           0.0    42.956341   \n",
       "\n",
       "  bkurto_bins7 class  \n",
       "0    33.029938     1  \n",
       "1    36.344060     1  \n",
       "2    35.631835     1  \n",
       "3    32.843335     1  \n",
       "4    36.139280     1  \n",
       "\n",
       "[5 rows x 106 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bins0</th>\n",
       "      <th>Bins1</th>\n",
       "      <th>Bins2</th>\n",
       "      <th>Bins3</th>\n",
       "      <th>Bins4</th>\n",
       "      <th>Bins5</th>\n",
       "      <th>Bins6</th>\n",
       "      <th>Bins7</th>\n",
       "      <th>rmean_bins0</th>\n",
       "      <th>rmean_bins1</th>\n",
       "      <th>...</th>\n",
       "      <th>bskew_bins7</th>\n",
       "      <th>bkurto_bins0</th>\n",
       "      <th>bkurto_bins1</th>\n",
       "      <th>bkurto_bins2</th>\n",
       "      <th>bkurto_bins3</th>\n",
       "      <th>bkurto_bins4</th>\n",
       "      <th>bkurto_bins5</th>\n",
       "      <th>bkurto_bins6</th>\n",
       "      <th>bkurto_bins7</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5822</td>\n",
       "      <td>14402</td>\n",
       "      <td>0</td>\n",
       "      <td>9189</td>\n",
       "      <td>10538</td>\n",
       "      <td>0</td>\n",
       "      <td>11831</td>\n",
       "      <td>13754</td>\n",
       "      <td>86.982480</td>\n",
       "      <td>4.430912</td>\n",
       "      <td>...</td>\n",
       "      <td>30.9217012</td>\n",
       "      <td>27.556681</td>\n",
       "      <td>26.125116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.668177</td>\n",
       "      <td>19.678833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.325143</td>\n",
       "      <td>33.029938</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6325</td>\n",
       "      <td>11758</td>\n",
       "      <td>0</td>\n",
       "      <td>10893</td>\n",
       "      <td>13314</td>\n",
       "      <td>0</td>\n",
       "      <td>8460</td>\n",
       "      <td>14786</td>\n",
       "      <td>86.937708</td>\n",
       "      <td>5.183279</td>\n",
       "      <td>...</td>\n",
       "      <td>32.45653958</td>\n",
       "      <td>30.429971</td>\n",
       "      <td>21.410499</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.701043</td>\n",
       "      <td>16.311605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.183742</td>\n",
       "      <td>36.344060</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5911</td>\n",
       "      <td>15666</td>\n",
       "      <td>0</td>\n",
       "      <td>9025</td>\n",
       "      <td>13587</td>\n",
       "      <td>0</td>\n",
       "      <td>8205</td>\n",
       "      <td>13142</td>\n",
       "      <td>76.833362</td>\n",
       "      <td>2.389570</td>\n",
       "      <td>...</td>\n",
       "      <td>32.04297751</td>\n",
       "      <td>31.149385</td>\n",
       "      <td>23.374875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.936013</td>\n",
       "      <td>17.187983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.580706</td>\n",
       "      <td>35.631835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5688</td>\n",
       "      <td>15515</td>\n",
       "      <td>0</td>\n",
       "      <td>8941</td>\n",
       "      <td>10462</td>\n",
       "      <td>0</td>\n",
       "      <td>11009</td>\n",
       "      <td>13921</td>\n",
       "      <td>73.869726</td>\n",
       "      <td>1.710925</td>\n",
       "      <td>...</td>\n",
       "      <td>29.69319943</td>\n",
       "      <td>31.537897</td>\n",
       "      <td>22.614153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.457708</td>\n",
       "      <td>15.659193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.081886</td>\n",
       "      <td>32.843335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6101</td>\n",
       "      <td>12231</td>\n",
       "      <td>0</td>\n",
       "      <td>10258</td>\n",
       "      <td>14797</td>\n",
       "      <td>0</td>\n",
       "      <td>7655</td>\n",
       "      <td>14494</td>\n",
       "      <td>87.165219</td>\n",
       "      <td>5.596517</td>\n",
       "      <td>...</td>\n",
       "      <td>32.21430004</td>\n",
       "      <td>31.326113</td>\n",
       "      <td>21.152459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.285766</td>\n",
       "      <td>13.344069</td>\n",
       "      <td>0.0</td>\n",
       "      <td>42.956341</td>\n",
       "      <td>36.139280</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bins0  Bins1  Bins2  Bins3  Bins4  Bins5  Bins6  Bins7  rmean_bins0  \\\n",
       "0   5822  14402      0   9189  10538      0  11831  13754    86.982480   \n",
       "1   6325  11758      0  10893  13314      0   8460  14786    86.937708   \n",
       "2   5911  15666      0   9025  13587      0   8205  13142    76.833362   \n",
       "3   5688  15515      0   8941  10462      0  11009  13921    73.869726   \n",
       "4   6101  12231      0  10258  14797      0   7655  14494    87.165219   \n",
       "\n",
       "   rmean_bins1  ...  bskew_bins7  bkurto_bins0  bkurto_bins1  bkurto_bins2  \\\n",
       "0     4.430912  ...   30.9217012     27.556681     26.125116           0.0   \n",
       "1     5.183279  ...  32.45653958     30.429971     21.410499           0.0   \n",
       "2     2.389570  ...  32.04297751     31.149385     23.374875           0.0   \n",
       "3     1.710925  ...  29.69319943     31.537897     22.614153           0.0   \n",
       "4     5.596517  ...  32.21430004     31.326113     21.152459           0.0   \n",
       "\n",
       "   bkurto_bins3  bkurto_bins4 bkurto_bins5 bkurto_bins6 bkurto_bins7 class  \n",
       "0     12.668177     19.678833          0.0    42.325143    33.029938     1  \n",
       "1      9.701043     16.311605          0.0    41.183742    36.344060     1  \n",
       "2     12.936013     17.187983          0.0    45.580706    35.631835     1  \n",
       "3     11.457708     15.659193          0.0    42.081886    32.843335     1  \n",
       "4     12.285766     13.344069          0.0    42.956341    36.139280     1  \n",
       "\n",
       "[5 rows x 105 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['filename'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['rstd_bins0','rstd_bins1','rstd_bins2','rstd_bins3','rstd_bins4','rstd_bins5','rstd_bins6','rstd_bins7','rskew_bins0','rskew_bins1','rskew_bins2','rskew_bins3','rskew_bins4','rskew_bins5','rskew_bins6','rskew_bins7','gstd_bins0','gstd_bins1','gstd_bins2','gstd_bins3','gstd_bins4','gstd_bins5','gstd_bins6','gstd_bins7','bstd_bins0','bstd_bins1','bstd_bins2','bstd_bins3','bstd_bins4','bstd_bins5','bstd_bins6','bstd_bins7','gskew_bins0','gskew_bins1','gskew_bins2','gskew_bins3','gskew_bins4','gskew_bins5','gskew_bins6','gskew_bins7','bskew_bins0','bskew_bins1','bskew_bins2','bskew_bins3','bskew_bins4','bskew_bins5','bskew_bins6','bskew_bins7','class']\n",
    "for i in name:\n",
    "    data[i] = pd.to_numeric(data[i],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bins0</th>\n",
       "      <th>Bins1</th>\n",
       "      <th>Bins2</th>\n",
       "      <th>Bins3</th>\n",
       "      <th>Bins4</th>\n",
       "      <th>Bins5</th>\n",
       "      <th>Bins6</th>\n",
       "      <th>Bins7</th>\n",
       "      <th>rmean_bins0</th>\n",
       "      <th>rmean_bins1</th>\n",
       "      <th>...</th>\n",
       "      <th>bskew_bins7</th>\n",
       "      <th>bkurto_bins0</th>\n",
       "      <th>bkurto_bins1</th>\n",
       "      <th>bkurto_bins2</th>\n",
       "      <th>bkurto_bins3</th>\n",
       "      <th>bkurto_bins4</th>\n",
       "      <th>bkurto_bins5</th>\n",
       "      <th>bkurto_bins6</th>\n",
       "      <th>bkurto_bins7</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5822.0</td>\n",
       "      <td>14402.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9189.0</td>\n",
       "      <td>10538.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11831.0</td>\n",
       "      <td>13754.0</td>\n",
       "      <td>86.982480</td>\n",
       "      <td>4.430912</td>\n",
       "      <td>...</td>\n",
       "      <td>30.921701</td>\n",
       "      <td>27.556681</td>\n",
       "      <td>26.125116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.668177</td>\n",
       "      <td>19.678833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.325143</td>\n",
       "      <td>33.029938</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6325.0</td>\n",
       "      <td>11758.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10893.0</td>\n",
       "      <td>13314.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8460.0</td>\n",
       "      <td>14786.0</td>\n",
       "      <td>86.937708</td>\n",
       "      <td>5.183279</td>\n",
       "      <td>...</td>\n",
       "      <td>32.456540</td>\n",
       "      <td>30.429971</td>\n",
       "      <td>21.410499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.701043</td>\n",
       "      <td>16.311605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>41.183742</td>\n",
       "      <td>36.344060</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5911.0</td>\n",
       "      <td>15666.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9025.0</td>\n",
       "      <td>13587.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8205.0</td>\n",
       "      <td>13142.0</td>\n",
       "      <td>76.833362</td>\n",
       "      <td>2.389570</td>\n",
       "      <td>...</td>\n",
       "      <td>32.042978</td>\n",
       "      <td>31.149385</td>\n",
       "      <td>23.374875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.936013</td>\n",
       "      <td>17.187983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.580706</td>\n",
       "      <td>35.631835</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5688.0</td>\n",
       "      <td>15515.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8941.0</td>\n",
       "      <td>10462.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11009.0</td>\n",
       "      <td>13921.0</td>\n",
       "      <td>73.869726</td>\n",
       "      <td>1.710925</td>\n",
       "      <td>...</td>\n",
       "      <td>29.693199</td>\n",
       "      <td>31.537897</td>\n",
       "      <td>22.614153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.457708</td>\n",
       "      <td>15.659193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.081886</td>\n",
       "      <td>32.843335</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6101.0</td>\n",
       "      <td>12231.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10258.0</td>\n",
       "      <td>14797.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7655.0</td>\n",
       "      <td>14494.0</td>\n",
       "      <td>87.165219</td>\n",
       "      <td>5.596517</td>\n",
       "      <td>...</td>\n",
       "      <td>32.214300</td>\n",
       "      <td>31.326113</td>\n",
       "      <td>21.152459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.285766</td>\n",
       "      <td>13.344069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>42.956341</td>\n",
       "      <td>36.139280</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2689</th>\n",
       "      <td>9870.0</td>\n",
       "      <td>10436.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9558.0</td>\n",
       "      <td>9764.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16080.0</td>\n",
       "      <td>9815.0</td>\n",
       "      <td>104.708207</td>\n",
       "      <td>27.440974</td>\n",
       "      <td>...</td>\n",
       "      <td>28.826381</td>\n",
       "      <td>16.609479</td>\n",
       "      <td>32.541509</td>\n",
       "      <td>10.841782</td>\n",
       "      <td>18.263777</td>\n",
       "      <td>29.591836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>43.584547</td>\n",
       "      <td>43.219779</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2690</th>\n",
       "      <td>5946.0</td>\n",
       "      <td>14026.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11041.0</td>\n",
       "      <td>12415.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7886.0</td>\n",
       "      <td>14221.0</td>\n",
       "      <td>74.044736</td>\n",
       "      <td>1.607016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.246127</td>\n",
       "      <td>30.936390</td>\n",
       "      <td>21.337923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.706518</td>\n",
       "      <td>17.877323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.339391</td>\n",
       "      <td>32.611797</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2691</th>\n",
       "      <td>7330.0</td>\n",
       "      <td>8408.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10811.0</td>\n",
       "      <td>18521.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6699.0</td>\n",
       "      <td>13759.0</td>\n",
       "      <td>112.515416</td>\n",
       "      <td>7.136774</td>\n",
       "      <td>...</td>\n",
       "      <td>28.790502</td>\n",
       "      <td>42.515393</td>\n",
       "      <td>18.625921</td>\n",
       "      <td>11.891740</td>\n",
       "      <td>14.170267</td>\n",
       "      <td>3.991819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.311970</td>\n",
       "      <td>41.914116</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>7630.0</td>\n",
       "      <td>16431.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9530.0</td>\n",
       "      <td>3413.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17843.0</td>\n",
       "      <td>10685.0</td>\n",
       "      <td>73.122412</td>\n",
       "      <td>24.310145</td>\n",
       "      <td>...</td>\n",
       "      <td>37.426827</td>\n",
       "      <td>20.622111</td>\n",
       "      <td>29.148814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.197666</td>\n",
       "      <td>31.678731</td>\n",
       "      <td>4.769168</td>\n",
       "      <td>50.967873</td>\n",
       "      <td>38.781249</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>5415.0</td>\n",
       "      <td>17347.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8790.0</td>\n",
       "      <td>12865.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8924.0</td>\n",
       "      <td>12193.0</td>\n",
       "      <td>60.702862</td>\n",
       "      <td>1.042082</td>\n",
       "      <td>...</td>\n",
       "      <td>34.588816</td>\n",
       "      <td>33.378388</td>\n",
       "      <td>26.261638</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.291031</td>\n",
       "      <td>20.163037</td>\n",
       "      <td>5.045378</td>\n",
       "      <td>45.099627</td>\n",
       "      <td>35.944590</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2683 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Bins0    Bins1  Bins2    Bins3    Bins4  Bins5    Bins6    Bins7  \\\n",
       "0     5822.0  14402.0    0.0   9189.0  10538.0    0.0  11831.0  13754.0   \n",
       "1     6325.0  11758.0    0.0  10893.0  13314.0    0.0   8460.0  14786.0   \n",
       "2     5911.0  15666.0    0.0   9025.0  13587.0    0.0   8205.0  13142.0   \n",
       "3     5688.0  15515.0    0.0   8941.0  10462.0    0.0  11009.0  13921.0   \n",
       "4     6101.0  12231.0    0.0  10258.0  14797.0    0.0   7655.0  14494.0   \n",
       "...      ...      ...    ...      ...      ...    ...      ...      ...   \n",
       "2689  9870.0  10436.0   13.0   9558.0   9764.0    0.0  16080.0   9815.0   \n",
       "2690  5946.0  14026.0    1.0  11041.0  12415.0    0.0   7886.0  14221.0   \n",
       "2691  7330.0   8408.0    6.0  10811.0  18521.0    2.0   6699.0  13759.0   \n",
       "2692  7630.0  16431.0    1.0   9530.0   3413.0    3.0  17843.0  10685.0   \n",
       "2693  5415.0  17347.0    0.0   8790.0  12865.0    2.0   8924.0  12193.0   \n",
       "\n",
       "      rmean_bins0  rmean_bins1  ...  bskew_bins7  bkurto_bins0  bkurto_bins1  \\\n",
       "0       86.982480     4.430912  ...    30.921701     27.556681     26.125116   \n",
       "1       86.937708     5.183279  ...    32.456540     30.429971     21.410499   \n",
       "2       76.833362     2.389570  ...    32.042978     31.149385     23.374875   \n",
       "3       73.869726     1.710925  ...    29.693199     31.537897     22.614153   \n",
       "4       87.165219     5.596517  ...    32.214300     31.326113     21.152459   \n",
       "...           ...          ...  ...          ...           ...           ...   \n",
       "2689   104.708207    27.440974  ...    28.826381     16.609479     32.541509   \n",
       "2690    74.044736     1.607016  ...    39.246127     30.936390     21.337923   \n",
       "2691   112.515416     7.136774  ...    28.790502     42.515393     18.625921   \n",
       "2692    73.122412    24.310145  ...    37.426827     20.622111     29.148814   \n",
       "2693    60.702862     1.042082  ...    34.588816     33.378388     26.261638   \n",
       "\n",
       "      bkurto_bins2  bkurto_bins3  bkurto_bins4  bkurto_bins5  bkurto_bins6  \\\n",
       "0         0.000000     12.668177     19.678833      0.000000     42.325143   \n",
       "1         0.000000      9.701043     16.311605      0.000000     41.183742   \n",
       "2         0.000000     12.936013     17.187983      0.000000     45.580706   \n",
       "3         0.000000     11.457708     15.659193      0.000000     42.081886   \n",
       "4         0.000000     12.285766     13.344069      0.000000     42.956341   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "2689     10.841782     18.263777     29.591836      0.000000     43.584547   \n",
       "2690      0.000000      9.706518     17.877323      0.000000     45.339391   \n",
       "2691     11.891740     14.170267      3.991819      0.000000     36.311970   \n",
       "2692      0.000000     20.197666     31.678731      4.769168     50.967873   \n",
       "2693      0.000000     17.291031     20.163037      5.045378     45.099627   \n",
       "\n",
       "      bkurto_bins7  class  \n",
       "0        33.029938    1.0  \n",
       "1        36.344060    1.0  \n",
       "2        35.631835    1.0  \n",
       "3        32.843335    1.0  \n",
       "4        36.139280    1.0  \n",
       "...            ...    ...  \n",
       "2689     43.219779    2.0  \n",
       "2690     32.611797    2.0  \n",
       "2691     41.914116    2.0  \n",
       "2692     38.781249    2.0  \n",
       "2693     35.944590    2.0  \n",
       "\n",
       "[2683 rows x 105 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "    return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "clean_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1878, 104), (805, 104))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['class'], axis=1),\n",
    "    data['class'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.09973137e+00, 1.44157726e+00, 4.85401101e+01, 4.40809978e-01,\n",
       "        5.54848427e+01, 2.55627741e-01, 1.76054866e-02, 6.63092512e+01,\n",
       "        2.15756342e+01, 6.53412092e-01, 7.31551304e+01, 1.67102192e+00,\n",
       "        1.78241634e+02, 1.24140637e+01, 4.63673414e+01, 3.20458765e+01,\n",
       "        9.13668484e+01, 1.54125448e+01, 7.75531157e+01, 4.03943045e+01,\n",
       "        5.60659710e+01, 3.72167947e+00, 9.39009093e+01, 1.33055197e+01,\n",
       "        1.03573105e+02, 1.51549348e+01, 7.00777098e+01, 3.52935399e+01,\n",
       "        4.69854519e+01, 5.22456538e+00, 8.23158122e+01, 1.28361994e+01,\n",
       "        1.22551889e+02, 1.34253863e+01, 6.62276320e+01, 3.02787007e+01,\n",
       "        3.90810218e+01, 5.86111562e+00, 6.87896616e+01, 1.40823895e+01,\n",
       "        1.08862090e+02, 6.39511147e+01, 8.52054745e+01, 1.23277179e+02,\n",
       "        1.24453993e+01, 1.44502485e+01, 4.72914712e+00, 8.78932391e+01,\n",
       "        2.88176570e+02, 5.63531569e+01, 7.72949798e+01, 2.81867046e+00,\n",
       "        5.23323261e+00, 3.23401545e-01, 3.32007069e+01, 9.75696681e+01,\n",
       "        2.63051233e+02, 1.82668156e+01, 6.90410807e+01, 3.46153788e+00,\n",
       "        2.01998833e+00, 1.84342736e-01, 3.51767164e+01, 1.05877280e+02,\n",
       "        1.87979406e+02, 1.84280003e+00, 6.66327381e+01, 6.49440690e+00,\n",
       "        1.73969748e+00, 1.00049838e+00, 3.65114671e+01, 1.14115007e+02,\n",
       "        1.87349110e+01, 2.48560316e+01, 8.31190995e+01, 1.30248169e+02,\n",
       "        4.79026921e+00, 1.19707767e+01, 5.42467759e+01, 1.41749879e+00,\n",
       "        1.55242916e+02, 9.57396264e+01, 8.25857145e+01, 7.08229793e+00,\n",
       "        3.07673401e+01, 3.03698014e+00, 5.77242629e+01, 5.86660802e+00,\n",
       "        1.57477316e+02, 7.22714571e+01, 6.76545087e+01, 4.84152297e-01,\n",
       "        2.57963777e+01, 3.73001492e+00, 5.95782006e+01, 1.60703235e-01,\n",
       "        1.55701691e+02, 5.07029773e+01, 6.39298745e+01, 1.23035945e+01,\n",
       "        1.56012058e+01, 5.71158541e+00, 6.06075550e+01, 1.97780182e+00]),\n",
       " array([7.84681344e-02, 2.30035855e-01, 4.46234568e-12, 5.06813358e-01,\n",
       "        1.42884105e-13, 6.13199392e-01, 8.94456207e-01, 6.95158768e-16,\n",
       "        3.63783264e-06, 4.18997894e-01, 2.44791405e-17, 1.96280895e-01,\n",
       "        6.59101458e-39, 4.36316881e-04, 1.31538353e-11, 1.73839564e-08,\n",
       "        3.59029434e-21, 8.95344146e-05, 2.87602424e-18, 2.59890980e-10,\n",
       "        1.07229529e-13, 5.38609940e-02, 1.05978791e-21, 2.71850044e-04,\n",
       "        1.02358802e-23, 1.02503258e-04, 1.09956912e-16, 3.37179900e-09,\n",
       "        9.66931548e-12, 2.23810826e-02, 2.84933225e-19, 3.48649123e-04,\n",
       "        1.22834179e-27, 2.55134891e-04, 7.23524921e-16, 4.25642885e-08,\n",
       "        5.02062081e-10, 1.55733364e-02, 2.06401261e-16, 1.80294456e-04,\n",
       "        8.18723264e-25, 2.20973608e-15, 7.03196242e-20, 8.71618623e-28,\n",
       "        4.29104979e-04, 1.48506773e-04, 2.97801627e-02, 1.91811850e-20,\n",
       "        3.08634437e-60, 9.30520799e-14, 3.26063400e-18, 9.33400851e-02,\n",
       "        2.22701492e-02, 5.69638645e-01, 9.69442539e-09, 1.81754697e-22,\n",
       "        1.83504877e-55, 2.01653253e-05, 1.82519480e-16, 6.29682370e-02,\n",
       "        1.55406538e-01, 6.67716725e-01, 3.57628879e-09, 3.40248321e-24,\n",
       "        7.62177740e-41, 1.74786395e-01, 5.93270135e-16, 1.09005148e-02,\n",
       "        1.87338137e-01, 3.17318941e-01, 1.82596366e-09, 6.71401080e-26,\n",
       "        1.58120432e-05, 6.74643411e-07, 1.93066169e-19, 3.24635798e-29,\n",
       "        2.87438577e-02, 5.52500069e-04, 2.63502293e-13, 2.33966158e-01,\n",
       "        2.70807087e-34, 4.37756325e-22, 2.49998468e-19, 7.85131234e-03,\n",
       "        3.32210998e-08, 8.15510462e-02, 4.73022771e-14, 1.55249738e-02,\n",
       "        9.59331702e-35, 3.76700283e-17, 3.59704500e-16, 4.86633679e-01,\n",
       "        4.17044498e-07, 5.35933957e-02, 1.89700680e-14, 6.88555540e-01,\n",
       "        2.18815820e-34, 1.52451093e-12, 2.23289499e-15, 4.62732412e-04,\n",
       "        8.10965714e-05, 1.69514925e-02, 1.14286617e-14, 1.59787789e-01]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the univariate statistical measure between\n",
    "# each of the variables and the target\n",
    "\n",
    "# similarly to chi2, the output is one array with f-scores\n",
    "# and one array with the pvalues\n",
    "\n",
    "univariate = f_classif(X_train, y_train)\n",
    "\n",
    "univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x215d4f6ddf0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) let's capture the pvalues in a pandas series\n",
    "# 2) add the variable names in the index\n",
    "# 3) sort the features based on their anova pvalues\n",
    "# 4) and make a var plot\n",
    "\n",
    "univariate = pd.Series(univariate[1])\n",
    "univariate.index = X_train.columns\n",
    "univariate.sort_values(ascending=False).plot.bar(figsize=(20, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rmean_bins4', 'rstd_bins0', 'rstd_bins6', 'rskew_bins0',\n",
       "       'rkurto_bins0', 'gmean_bins0', 'gmean_bins2', 'gmean_bins3',\n",
       "       'gmean_bins7', 'gstd_bins0', 'gstd_bins7', 'gskew_bins0', 'gskew_bins7',\n",
       "       'gkurto_bins0', 'gkurto_bins7', 'bmean_bins3', 'bstd_bins0',\n",
       "       'bstd_bins1', 'bskew_bins0', 'bkurto_bins0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the top 10 features\n",
    "sel_ = SelectKBest(f_classif, k=20).fit(X_train, y_train)\n",
    "\n",
    "# display selected feature names\n",
    "X_train.columns[sel_.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I keep a copy of the dataset with all the variables\n",
    "# to compare the performance of machine learning models\n",
    "# at the end of the notebook\n",
    "\n",
    "X_train_original = X_train.copy()\n",
    "X_test_original = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correlated features:  39\n"
     ]
    }
   ],
   "source": [
    "# find and remove correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    \n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            # we are interested in absolute coeff value\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    \n",
    "    return col_corr\n",
    "\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1878, 65), (805, 65))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove correlated features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep a copy of the dataset at  this stage\n",
    "X_train_corr = X_train.copy()\n",
    "X_test_corr = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1878, 20), (805, 20))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel_ = SelectKBest(f_classif, k=20).fit(X_train, y_train)\n",
    "\n",
    "# capture selected feature names\n",
    "features_to_keep = X_train.columns[sel_.get_support()]\n",
    "\n",
    "# select features\n",
    "X_train_anova = sel_.transform(X_train)\n",
    "X_test_anova = sel_.transform(X_test)\n",
    "\n",
    "# numpy array to dataframe\n",
    "X_train_anova = pd.DataFrame(X_train_anova)\n",
    "X_train_anova.columns = features_to_keep\n",
    "\n",
    "X_test_anova = pd.DataFrame(X_test_anova)\n",
    "X_test_anova.columns = features_to_keep\n",
    "\n",
    "X_train_anova.shape, X_test_anova.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rmean_bins2', 'rmean_bins4', 'rstd_bins0', 'rstd_bins2', 'rstd_bins6',\n",
       "       'rskew_bins0', 'rskew_bins2', 'rskew_bins6', 'gmean_bins0',\n",
       "       'gstd_bins0', 'gstd_bins2', 'gstd_bins7', 'gskew_bins0', 'gskew_bins2',\n",
       "       'bstd_bins0', 'bstd_bins1', 'bstd_bins2', 'bskew_bins0', 'bskew_bins1',\n",
       "       'bskew_bins2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build random forests and\n",
    "# compare its performance in train and test sets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import scikitplot as skplt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_randomForests(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    print('Train set')\n",
    "    pred = rf.predict_proba(X_train)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    \n",
    "    print('Test set')\n",
    "    pred = rf.predict_proba(X_test)\n",
    "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print('Metrics:')\n",
    "    print('Accuracy: {0:.3f}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print('F1 Score: {0:.3f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "    print('Precision: {0:.3f}'.format(metrics.precision_score(y_test, y_pred)))\n",
    "    print('Recall: {0:.3f}'.format(metrics.recall_score(y_test, y_pred)))\n",
    "    \n",
    "    print('After Cross Validation:')\n",
    "    accuracies = cross_val_score(estimator = rf, X = X_train, y = y_train, cv = 10)\n",
    "    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.9221462274851412\n",
      "Test set\n",
      "Random Forests roc-auc: 0.9003296418385867\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.80      0.88      0.84       406\n",
      "         2.0       0.87      0.78      0.82       399\n",
      "\n",
      "    accuracy                           0.83       805\n",
      "   macro avg       0.83      0.83      0.83       805\n",
      "weighted avg       0.83      0.83      0.83       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[358  48]\n",
      " [ 89 310]]\n",
      "Metrics:\n",
      "Accuracy: 0.830\n",
      "F1 Score: 0.839\n",
      "Precision: 0.801\n",
      "Recall: 0.882\n",
      "After Cross Validation:\n",
      "Accuracy: 81.31 %\n",
      "Standard Deviation: 3.46 %\n"
     ]
    }
   ],
   "source": [
    "run_randomForests(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Random Forests roc-auc: 0.8930833446758313\n",
      "Test set\n",
      "Random Forests roc-auc: 0.8732699976542341\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.78      0.84      0.81       406\n",
      "         2.0       0.82      0.76      0.79       399\n",
      "\n",
      "    accuracy                           0.80       805\n",
      "   macro avg       0.80      0.80      0.80       805\n",
      "weighted avg       0.80      0.80      0.80       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[340  66]\n",
      " [ 95 304]]\n",
      "Metrics:\n",
      "Accuracy: 0.800\n",
      "F1 Score: 0.809\n",
      "Precision: 0.782\n",
      "Recall: 0.837\n",
      "After Cross Validation:\n",
      "Accuracy: 77.85 %\n",
      "Standard Deviation: 4.34 %\n"
     ]
    }
   ],
   "source": [
    "# filter methods - univariate roc-auc\n",
    "run_randomForests(X_train_anova,\n",
    "                  X_test_anova,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def run_logistic(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    \n",
    "    # function to train and test the performance of logistic regression\n",
    "    logit = LogisticRegression(penalty='l1', random_state=44, max_iter=1000, solver='liblinear')\n",
    "    logit.fit(X_train, y_train)\n",
    "    \n",
    "    print('Train set')\n",
    "    pred = logit.predict_proba(scaler.transform(X_train))\n",
    "    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    \n",
    "    print('Test set')\n",
    "    pred = logit.predict_proba(scaler.transform(X_test))\n",
    "    print('Logistic Regression roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n",
    "    \n",
    "    y_pred = logit.predict(X_test)\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print('Metrics:')\n",
    "    print('Accuracy: {0:.3f}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print('F1 Score: {0:.3f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "    print('Precision: {0:.3f}'.format(metrics.precision_score(y_test, y_pred)))\n",
    "    print('Recall: {0:.3f}'.format(metrics.recall_score(y_test, y_pred)))\n",
    "    \n",
    "    print('After Cross Validation:')\n",
    "    accuracies = cross_val_score(estimator = logit, X = X_train, y = y_train, cv = 10)\n",
    "    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.6962467673880496\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.6918404385347605\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.85      0.90      0.87       406\n",
      "         2.0       0.89      0.83      0.86       399\n",
      "\n",
      "    accuracy                           0.87       805\n",
      "   macro avg       0.87      0.87      0.87       805\n",
      "weighted avg       0.87      0.87      0.87       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[366  40]\n",
      " [ 67 332]]\n",
      "Metrics:\n",
      "Accuracy: 0.867\n",
      "F1 Score: 0.872\n",
      "Precision: 0.845\n",
      "Recall: 0.901\n",
      "After Cross Validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.25 %\n",
      "Standard Deviation: 2.42 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "run_logistic(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Logistic Regression roc-auc: 0.783228982351073\n",
      "Test set\n",
      "Logistic Regression roc-auc: 0.7773682975912687\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.82      0.77       406\n",
      "         2.0       0.78      0.68      0.73       399\n",
      "\n",
      "    accuracy                           0.75       805\n",
      "   macro avg       0.75      0.75      0.75       805\n",
      "weighted avg       0.75      0.75      0.75       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[332  74]\n",
      " [129 270]]\n",
      "Metrics:\n",
      "Accuracy: 0.748\n",
      "F1 Score: 0.766\n",
      "Precision: 0.720\n",
      "Recall: 0.818\n",
      "After Cross Validation:\n",
      "Accuracy: 73.32 %\n",
      "Standard Deviation: 2.65 %\n"
     ]
    }
   ],
   "source": [
    "run_logistic(X_train_anova,\n",
    "                  X_test_anova,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build logistic regression\n",
    "# and compare its performance in train and test sets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def run_kernel_SVM(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    \n",
    "    # function to train and test the performance of logistic regression\n",
    "    classifier = SVC(kernel = 'rbf', random_state = 0, probability=True)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    print('Train set')\n",
    "    pred = classifier.predict_proba(scaler.transform(X_train))\n",
    "    print('Kernel SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    \n",
    "    print('Test set')\n",
    "    pred = classifier.predict_proba(scaler.transform(X_test))\n",
    "    print('Kernel SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print('Metrics:')\n",
    "    print('Accuracy: {0:.3f}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print('F1 Score: {0:.3f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "    print('Precision: {0:.3f}'.format(metrics.precision_score(y_test, y_pred)))\n",
    "    print('Recall: {0:.3f}'.format(metrics.recall_score(y_test, y_pred)))\n",
    "    \n",
    "    print('After Cross Validation:')\n",
    "    accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Kernel SVM roc-auc: 0.7122181389229163\n",
      "Test set\n",
      "Kernel SVM roc-auc: 0.7524599676531228\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.63      0.88      0.74       406\n",
      "         2.0       0.80      0.48      0.60       399\n",
      "\n",
      "    accuracy                           0.68       805\n",
      "   macro avg       0.72      0.68      0.67       805\n",
      "weighted avg       0.72      0.68      0.67       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[358  48]\n",
      " [208 191]]\n",
      "Metrics:\n",
      "Accuracy: 0.682\n",
      "F1 Score: 0.737\n",
      "Precision: 0.633\n",
      "Recall: 0.882\n",
      "After Cross Validation:\n",
      "Accuracy: 65.01 %\n",
      "Standard Deviation: 1.96 %\n"
     ]
    }
   ],
   "source": [
    "run_kernel_SVM(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Kernel SVM roc-auc: 0.7797695204391815\n",
      "Test set\n",
      "Kernel SVM roc-auc: 0.805085373532353\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.87      0.79       406\n",
      "         2.0       0.84      0.65      0.73       399\n",
      "\n",
      "    accuracy                           0.76       805\n",
      "   macro avg       0.78      0.76      0.76       805\n",
      "weighted avg       0.78      0.76      0.76       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[355  51]\n",
      " [139 260]]\n",
      "Metrics:\n",
      "Accuracy: 0.764\n",
      "F1 Score: 0.789\n",
      "Precision: 0.719\n",
      "Recall: 0.874\n",
      "After Cross Validation:\n",
      "Accuracy: 72.63 %\n",
      "Standard Deviation: 2.40 %\n"
     ]
    }
   ],
   "source": [
    "run_kernel_SVM(X_train_anova,\n",
    "                  X_test_anova,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build logistic regression\n",
    "# and compare its performance in train and test sets\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def run_knn(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    \n",
    "    # function to train and test the performance of logistic regression\n",
    "    classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    print('Train set')\n",
    "    pred = classifier.predict_proba(scaler.transform(X_train))\n",
    "    print('KNN roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    \n",
    "    print('Test set')\n",
    "    pred = classifier.predict_proba(scaler.transform(X_test))\n",
    "    print('KNN roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print('Metrics:')\n",
    "    print('Accuracy: {0:.3f}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print('F1 Score: {0:.3f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "    print('Precision: {0:.3f}'.format(metrics.precision_score(y_test, y_pred)))\n",
    "    print('Recall: {0:.3f}'.format(metrics.recall_score(y_test, y_pred)))\n",
    "    \n",
    "    print('After Cross Validation:')\n",
    "    accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "KNN roc-auc: 0.5\n",
      "Test set\n",
      "KNN roc-auc: 0.5\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.83      0.77       406\n",
      "         2.0       0.80      0.67      0.73       399\n",
      "\n",
      "    accuracy                           0.75       805\n",
      "   macro avg       0.76      0.75      0.75       805\n",
      "weighted avg       0.76      0.75      0.75       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[338  68]\n",
      " [131 268]]\n",
      "Metrics:\n",
      "Accuracy: 0.753\n",
      "F1 Score: 0.773\n",
      "Precision: 0.721\n",
      "Recall: 0.833\n",
      "After Cross Validation:\n",
      "Accuracy: 71.94 %\n",
      "Standard Deviation: 3.37 %\n"
     ]
    }
   ],
   "source": [
    "run_knn(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "KNN roc-auc: 0.5\n",
      "Test set\n",
      "KNN roc-auc: 0.5\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.87      0.79       406\n",
      "         2.0       0.83      0.65      0.73       399\n",
      "\n",
      "    accuracy                           0.76       805\n",
      "   macro avg       0.77      0.76      0.76       805\n",
      "weighted avg       0.77      0.76      0.76       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[353  53]\n",
      " [139 260]]\n",
      "Metrics:\n",
      "Accuracy: 0.761\n",
      "F1 Score: 0.786\n",
      "Precision: 0.717\n",
      "Recall: 0.869\n",
      "After Cross Validation:\n",
      "Accuracy: 76.46 %\n",
      "Standard Deviation: 2.29 %\n"
     ]
    }
   ],
   "source": [
    "run_knn(X_train_anova,\n",
    "                  X_test_anova,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def run_decision_tree(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # function to train and test the performance of logistic regression\n",
    "    classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    print('Train set')\n",
    "    pred = classifier.predict_proba(X_train)\n",
    "    print('Decision Tree roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    \n",
    "    print('Test set')\n",
    "    pred = classifier.predict_proba(X_test)\n",
    "    print('Decision Tree roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print('Metrics:')\n",
    "    print('Accuracy: {0:.3f}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print('F1 Score: {0:.3f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "    print('Precision: {0:.3f}'.format(metrics.precision_score(y_test, y_pred)))\n",
    "    print('Recall: {0:.3f}'.format(metrics.recall_score(y_test, y_pred)))\n",
    "    \n",
    "    print('After Cross Validation:')\n",
    "    accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Decision Tree roc-auc: 1.0\n",
      "Test set\n",
      "Decision Tree roc-auc: 0.7924336703828536\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.79      0.81      0.80       406\n",
      "         2.0       0.80      0.78      0.79       399\n",
      "\n",
      "    accuracy                           0.79       805\n",
      "   macro avg       0.79      0.79      0.79       805\n",
      "weighted avg       0.79      0.79      0.79       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[327  79]\n",
      " [ 88 311]]\n",
      "Metrics:\n",
      "Accuracy: 0.793\n",
      "F1 Score: 0.797\n",
      "Precision: 0.788\n",
      "Recall: 0.805\n",
      "After Cross Validation:\n",
      "Accuracy: 76.78 %\n",
      "Standard Deviation: 2.64 %\n"
     ]
    }
   ],
   "source": [
    "run_decision_tree(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Decision Tree roc-auc: 1.0\n",
      "Test set\n",
      "Decision Tree roc-auc: 0.7550773485437732\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.75      0.78      0.76       406\n",
      "         2.0       0.76      0.73      0.75       399\n",
      "\n",
      "    accuracy                           0.76       805\n",
      "   macro avg       0.76      0.76      0.76       805\n",
      "weighted avg       0.76      0.76      0.76       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[316  90]\n",
      " [107 292]]\n",
      "Metrics:\n",
      "Accuracy: 0.755\n",
      "F1 Score: 0.762\n",
      "Precision: 0.747\n",
      "Recall: 0.778\n",
      "After Cross Validation:\n",
      "Accuracy: 74.65 %\n",
      "Standard Deviation: 3.03 %\n"
     ]
    }
   ],
   "source": [
    "run_decision_tree(X_train_anova,\n",
    "                  X_test_anova,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build logistic regression\n",
    "# and compare its performance in train and test sets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def run_naive_bayes(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    \n",
    "    # function to train and test the performance of logistic regression\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    print('Train set')\n",
    "    pred = classifier.predict_proba(scaler.transform(X_train))\n",
    "    print('Kernel SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    \n",
    "    print('Test set')\n",
    "    pred = classifier.predict_proba(scaler.transform(X_test))\n",
    "    print('Kernel SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print('Metrics:')\n",
    "    print('Accuracy: {0:.3f}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print('F1 Score: {0:.3f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "    print('Precision: {0:.3f}'.format(metrics.precision_score(y_test, y_pred)))\n",
    "    print('Recall: {0:.3f}'.format(metrics.recall_score(y_test, y_pred)))\n",
    "    \n",
    "    print('After Cross Validation:')\n",
    "    accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Kernel SVM roc-auc: 0.5\n",
      "Test set\n",
      "Kernel SVM roc-auc: 0.5\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.72      0.88      0.80       406\n",
      "         2.0       0.85      0.66      0.74       399\n",
      "\n",
      "    accuracy                           0.77       805\n",
      "   macro avg       0.79      0.77      0.77       805\n",
      "weighted avg       0.79      0.77      0.77       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[359  47]\n",
      " [137 262]]\n",
      "Metrics:\n",
      "Accuracy: 0.771\n",
      "F1 Score: 0.796\n",
      "Precision: 0.724\n",
      "Recall: 0.884\n",
      "After Cross Validation:\n",
      "Accuracy: 73.59 %\n",
      "Standard Deviation: 3.70 %\n"
     ]
    }
   ],
   "source": [
    "run_naive_bayes(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set\n",
      "Kernel SVM roc-auc: 0.5\n",
      "Test set\n",
      "Kernel SVM roc-auc: 0.5\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.71      0.87      0.79       406\n",
      "         2.0       0.83      0.64      0.73       399\n",
      "\n",
      "    accuracy                           0.76       805\n",
      "   macro avg       0.77      0.76      0.76       805\n",
      "weighted avg       0.77      0.76      0.76       805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[355  51]\n",
      " [143 256]]\n",
      "Metrics:\n",
      "Accuracy: 0.759\n",
      "F1 Score: 0.785\n",
      "Precision: 0.713\n",
      "Recall: 0.874\n",
      "After Cross Validation:\n",
      "Accuracy: 73.43 %\n",
      "Standard Deviation: 2.77 %\n"
     ]
    }
   ],
   "source": [
    "run_naive_bayes(X_train_anova,\n",
    "                  X_test_anova,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to build logistic regression\n",
    "# and compare its performance in train and test sets\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def run_linear_SVM(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    \n",
    "    # function to train and test the performance of logistic regression\n",
    "    classifier = SVC(kernel = 'linear', random_state = 0, probability=True)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    print('Train set')\n",
    "    pred = classifier.predict_proba(scaler.transform(X_train))\n",
    "    print('Kernel SVM roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
    "    \n",
    "    print('Test set')\n",
    "    pred = classifier.predict_proba(scaler.transform(X_test))\n",
    "    print('Kernel SVM roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))\n",
    "    \n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    print('Classification Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    print('Metrics:')\n",
    "    print('Accuracy: {0:.3f}'.format(metrics.accuracy_score(y_test, y_pred)))\n",
    "    print('F1 Score: {0:.3f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "    print('Precision: {0:.3f}'.format(metrics.precision_score(y_test, y_pred)))\n",
    "    print('Recall: {0:.3f}'.format(metrics.recall_score(y_test, y_pred)))\n",
    "    \n",
    "    print('After Cross Validation:')\n",
    "    accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "    print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "    print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_linear_SVM(X_train_original,\n",
    "                  X_test_original,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_linear_SVM(X_train_anova,\n",
    "                  X_test_anova,\n",
    "                  y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
